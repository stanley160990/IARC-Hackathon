{"cells":[{"cell_type":"markdown","metadata":{"id":"3hwPgiYmgBKf"},"source":["# Pra-Pemrosesan Data Teks\n","\n","Salah satu Prinsip Utama untuk dipahami selama Pra-pemrosesan Data adalah memiliki Ide yang jelas tentang bagaimana tampilan data Input dan dan bagaimana kita ingin output akhir terlihat.\n","\n","Langkah-langkah yang diikuti untuk pre-processing adalah sebagai berikut :\n","\n","- Memahami Format Data\n","- Menyimpan data Topan ke dalam <i>Dictionary</i>\n","- Mengubah <i>Dictionary</i> menjadi <i>Dataframe</i>\n","- Restrukturisasi Kolom dan membuatnya mudah dibaca\n","- Mengganti Nilai Sentinel dan Menghapus String Kosong\n","- Menghapus Spasi yang Tidak Diinginkan dan Mengindeks ulang <i>dataframe</i>\n","- Simpan Dataframe ini ke File CSV\n"]},{"cell_type":"markdown","metadata":{"id":"k7kdXiOTgBKn"},"source":["### Memahami Format Data\n","\n","Mari kita lihat Modifikasi Format CSV yang digunakan oleh Tim HURDAT2 :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2qn6VwwbgBKo"},"outputs":[],"source":["from IPython.display import IFrame\n","IFrame(\"http://www.nhc.noaa.gov/data/hurdat/hurdat2-format-atlantic.pdf\", width=950, height=600)"]},{"cell_type":"markdown","metadata":{"id":"bmiVNAEKgBKq"},"source":["File telah diunduh dan sekarang mari kita baca file tersebut."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xB0ZyP6SgBKr"},"outputs":[],"source":["# Membuka File Data\n","atlantic = open(\"data/hurdat2-1851-2018-120319.txt\", \"r\")\n","atlantic_raw = atlantic.read()\n","\n","# Menjalankan penghitung (counter) untuk memeriksa dua huruf pertama dari Dokumen\n","import io\n","from collections import Counter\n","\n","c = Counter()\n","for line in io.StringIO(atlantic_raw):\n","    c[line[:2]] += 1\n","#Printing Counter Output\n","print(c)"]},{"cell_type":"markdown","metadata":{"id":"BrUhs7eRgBKt"},"source":["\n","\n","Memahami Apa Arti Output penghitung (<i>counter</i>) :\n","\n","* AL : Jumlah Badai Atlantik dari 1851-2018\n","* 18 : Jumlah Entri pada Abad ke-19 ( 1851 - 1899)\n","* 19 : Jumlah Entri pada Abad ke-20 ( 1900 - 1999)\n","* 20 : Jumlah Entri Abad 21 ( 2000 - 2018)\n"]},{"cell_type":"markdown","metadata":{"id":"ItAu6TmDgBKw"},"source":["### Menyimpan data Topan dalam <i>dictionary</i>\n","\n","Sekarang mari kita buat <i>Dictionary</i> untuk menyimpan data Topan sesuai dengan namanya."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gK8n-bYYgBKx"},"outputs":[],"source":["import io\n","\n","# Buat dictionary untuk Menyimpan Semua Data Topan Sesuai dengan namanya\n","atlantic_storms_r = []\n","atlantic_storm_r = {'header': None, 'data': []}\n","\n","for i, line in enumerate(io.StringIO(atlantic_raw)):\n","    if line[:2] == 'AL':\n","        atlantic_storms_r.append(atlantic_storm_r.copy())\n","        atlantic_storm_r['header'] = line\n","        atlantic_storm_r['data'] = []\n","    else:\n","        atlantic_storm_r['data'].append(line)\n","# Menghapus Elemen Pertama dari List dan Menyimpan Lainnya.\n","atlantic_storms_r = atlantic_storms_r[1:]\n","# Jumlah Topan Atlantik\n","len(atlantic_storms_r)"]},{"cell_type":"markdown","metadata":{"id":"-UnuwGoLgBKy"},"source":["### Mengubah <i>dictionary</i> menjadi <i>Dataframe</i>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbg1adGNgBKz"},"outputs":[],"source":["# Mengkonversi Dictionary menjadi Pandas Dataframe\n","\n","import pandas as pd\n","\n","atlantic_storm_dfs = []\n","for storm_dict in atlantic_storms_r:\n","    storm_id, storm_name, storm_entries_n = storm_dict['header'].split(\",\")[:3]\n","    # menghapus baris baru atau newline ('\\n'), dan memisahkan data dengan koma(,)\n","    data = [[entry.strip() for entry in datum[:-1].split(\",\")] for datum in storm_dict['data']]\n","    frame = pd.DataFrame(data)\n","    frame['id'] = storm_id\n","    frame['name'] = storm_name\n","    atlantic_storm_dfs.append(frame)\n","    \n","# Menampilkan dataframe Topan \n","atlantic_storm_dfs[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtYgwDf2gBK0"},"outputs":[],"source":["# Menggabungkan Semua Data Topan menjadi satu\n","atlantic_storms = pd.concat(atlantic_storm_dfs)\n","len(atlantic_storms)"]},{"cell_type":"markdown","metadata":{"id":"9CUkAzfBgBK1"},"source":["### Restrukturisasi Kolom dan membuatnya mudah dibaca"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adXqyLoZgBK2"},"outputs":[],"source":["# Restrukturisasi kolom pada Dataframe\n","atlantic_storms = atlantic_storms.reindex(columns=atlantic_storms.columns[-2:] | atlantic_storms.columns[:-2]) \n","# Mencetak 5 baris pertama\n","atlantic_storms.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXHXoFPKgBK4"},"outputs":[],"source":["#Menampilkan kolom-kolom yang ada di dataset\n","atlantic_storms.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ogxfl8RkgBK5"},"outputs":[],"source":["# Memproses kolom agar lebih mudah dibaca\n","atlantic_storms.columns = [\n","        \"id\",\n","        \"name\",\n","        \"date\",\n","        \"hours_minutes\",\n","        \"record_identifier\",\n","        \"status_of_system\",\n","        \"latitude\",\n","        \"longitude\",\n","        \"maximum_sustained_wind_knots\",\n","        \"maximum_pressure\",\n","        \"34_kt_ne\",\n","        \"34_kt_se\",\n","        \"34_kt_sw\",\n","        \"34_kt_nw\",\n","        \"50_kt_ne\",\n","        \"50_kt_se\",\n","        \"50_kt_sw\",\n","        \"50_kt_nw\",\n","        \"64_kt_ne\",\n","        \"64_kt_se\",\n","        \"64_kt_sw\",\n","        \"64_kt_nw\",\n","        \"na\"\n","]\n","del atlantic_storms['na']\n","pd.set_option(\"max_columns\", None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URb4yyiegBK7"},"outputs":[],"source":["# Melihat dataframe : \n","atlantic_storms.head()"]},{"cell_type":"markdown","metadata":{"id":"-aH0AVsSgBK7"},"source":["### Mengganti Nilai Sentinel dan Membuang String kosong\n","\n","Sekarang kita telah menyelesaikan sebagian besar Parsing , Mari kita lakukan beberapa perbaikan terakhir dengan mengubah nilai sentinel yaitu '-999' menjadi NaN ( Bukan angka )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0IB6MX5gBLC"},"outputs":[],"source":["# Mengganti semua nilai Sentinel (-999 ) dengan nan\n","atlantic_storms.iloc[0]['34_kt_sw']\n","\n","# Kita menggunakan Numpy ( Numerical Python ) untuk mengganti nilai Sentinel.\n","import numpy as np\n","atlantic_storms = atlantic_storms.replace(to_replace='-999', value=np.nan)\n","atlantic_storms.iloc[0]['34_kt_sw']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoEAKX8BgBLD"},"outputs":[],"source":["# Mengecek tipe data dari setiap Kolom\n","atlantic_storms.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXu2NxTPgBLE"},"outputs":[],"source":["atlantic_storms['record_identifier'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"o8HgJpWqgBLE"},"source":["Mengganti semua Empty Strings dengan NaN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"56_EoNYpgBLF"},"outputs":[],"source":["# Mengganti Semua String Kosong dengan nilai NaN\n","atlantic_storms = atlantic_storms.replace(to_replace=\"\", value=np.nan)\n","atlantic_storms['record_identifier'].value_counts(dropna=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsUUVM2TgBLF"},"outputs":[],"source":["#Melihat dataframe : \n","atlantic_storms.head()"]},{"cell_type":"markdown","metadata":{"id":"5NCZ3Dy_gBLG"},"source":["### Menghapus Spasi yang Tidak Diinginkan dan Mengindeks Ulang <i>dataframe</i>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jE3UXesHgBLG"},"outputs":[],"source":["# Perbaikan terakhir\n","\n","# Lepaskan Spasi yang Tidak Diinginkan dari nama\n","atlantic_storms['name'] = atlantic_storms['name'].map(lambda n: n.strip()) \n","\n","#ReIndex\n","atlantic_storms.index = range(len(atlantic_storms.index))\n","atlantic_storms.index.name = \"index\""]},{"cell_type":"markdown","metadata":{"id":"AeKZQ36OgBLG"},"source":["### Menyimpan Dataframe ini ke File CSV\n","\n","Mari kita simpan Dataframe ke dalam file CSV yang akan kita gunakan untuk Menganotasi Data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fbuJY6UfgBLH"},"outputs":[],"source":["atlantic_storms.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MY-NeTalgBLI"},"outputs":[],"source":["atlantic_storms.to_csv(\"atlantic.csv\")"]},{"cell_type":"markdown","metadata":{"id":"Wwwia7N6gBLL"},"source":["## License\n","\n","This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}